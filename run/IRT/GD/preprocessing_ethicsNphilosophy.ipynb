{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import argparse\n",
    "from scipy.stats import mode\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import json\n",
    "import warnings\n",
    "import itertools\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "##### subject list #####\n",
    "# 한국사: 한국사\n",
    "# 사회 탐구: 생활과 윤리, 정치와 법, 한국지리, 세계지리, 사회?문화, 동아시아사, 통합사회, 세계사, 윤리와 사상, 경제\n",
    "# 수학: 수학Ⅰ, 수학Ⅱ     ###  확률과 통계, 기하, 중학, 미적분, 고등수학, \n",
    "# 영어: 독해     ###  , 듣기\n",
    "# 과학 탐구: 화학Ⅰ, 통합과학, 물리학Ⅰ, 지구과학 Ⅰ, 생명과학 Ⅰ, 생명과학Ⅱ, 지구과학Ⅱ, 물리학Ⅱ, 화학Ⅱ\n",
    "# 국어: 독서, 문학            ### 언어와 매체, , 화법과 작문\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Define your parameters. as a dictionary\n",
    "parameters_dict = {\n",
    "    # \"subjects\": '생명과학Ⅰ',\n",
    "    # \"subjects\": '화학Ⅰ',\n",
    "    # \"subjects\": '독해,듣기',\n",
    "    # \"subjects\": '화학Ⅱ',\n",
    "    \"subjects\": '윤리와 사상',\n",
    "    # \"subjects\": '수학Ⅰ,수학Ⅱ',\n",
    "    # \"subjects\": '물리학Ⅱ',\n",
    "    # \"subjects\": '통합사회',\n",
    "    # \"subjects\": '정치와 법',\n",
    "    # \"subjects\": '독서,문학',\n",
    "    # \"subjects\": '한국사',\n",
    "    # \"subjects\": '사회?문화',\n",
    "    # \"subjects\": '동아시아사',\n",
    "    # \"subjects\": '한국사',\n",
    "    \"num_problems\": 10,\n",
    "    # \"point_constraints\": '2:1,3:1,4:1',\n",
    "    'set_type':['EC040003','EC040014'],\n",
    "    #\"subject_constraints\": '수학Ⅰ:5, 수학Ⅱ: 5',  # '물리학Ⅱ: 10',\n",
    "    \"seed\": 42,\n",
    "    \"original_data_path\": '../../../data/full_data',\n",
    "    \"same_standard\":  'middle_category',  # 'small_category', #\n",
    "    \"IQR_range\": 1.5\n",
    "}\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "\n",
    "# Convert dictionary to SimpleNamespace\n",
    "parameters = SimpleNamespace(**parameters_dict)\n",
    "set_seed(parameters.seed)\n",
    "\n",
    "\n",
    "###### import original datasets\n",
    "category = pd.read_csv(f'{parameters.original_data_path}/perfectexam_category_202404031323.csv', encoding='cp949')\n",
    "problem = pd.read_csv(f'{parameters.original_data_path}/perfectexam_problem_202404031323.csv', encoding='cp949')\n",
    "member_exam_answer = pd.read_csv(f'{parameters.original_data_path}/perfectexam_member_exam_answer_202404031323.csv', encoding='cp949')\n",
    "member_exam = pd.read_csv(f'{parameters.original_data_path}/perfectexam_member_exam_202404031323.csv', encoding='cp949')\n",
    "##### 기탭에서 답변에 따라 ######\n",
    "exam_in_range = pd.read_csv(f'{parameters.original_data_path}/perfectexam_exam_202404031323.csv', encoding='cp949')\n",
    "\n",
    "category_simple = category[['code', 'code_name']]\n",
    "problem_simple = problem[['seq', 'subject', 'big_category', 'middle_category','small_category','ref_seq','point','answer_rate',\"set_type\", \"type1\", \"type2\", \"num1\", \"ref_num\", \"represent_yn\"]]\n",
    "member_exam_answer_simple = member_exam_answer[['member_idx', 'member_exam_seq', 'problem_seq', 'answer_dttm','correct_yn']]\n",
    "member_exam_simple = member_exam[['seq', 'exam_seq', 'title']]\n",
    "first_merge = member_exam_answer_simple.merge(member_exam_simple, left_on='member_exam_seq', right_on='seq', how='left').drop(columns='seq')\n",
    "second_merge = first_merge[first_merge['exam_seq'].isin(exam_in_range['seq'])]\n",
    "third_merge = second_merge.merge(exam_in_range[['seq', 'title']], left_on='exam_seq', right_on='seq', how='left').drop(columns=['seq', 'exam_seq', 'title_x'])\n",
    "member_exam_answer_in_range = third_merge.merge(problem_simple, left_on='problem_seq', right_on='seq', how='left').drop(columns='seq')\n",
    "category_dict = pd.Series(category_simple['code_name'].values, index=category_simple['code']).to_dict()\n",
    "for column in ['subject', 'big_category', 'middle_category','small_category']:\n",
    "    member_exam_answer_in_range[column] = member_exam_answer_in_range[column].map(category_dict)\n",
    "\n",
    "###### filter by subjects\n",
    "subjects_list = parameters.subjects.split(',')\n",
    "filtered_data = member_exam_answer_in_range[member_exam_answer_in_range['subject'].isin(subjects_list)]\n",
    "###### get first attemps\n",
    "filtered_data_sorted = filtered_data.sort_values(by=['member_idx', 'problem_seq', 'answer_dttm'])\n",
    "first_attempts = filtered_data_sorted.groupby(['member_idx', 'problem_seq']).last().reset_index()\n",
    "\n",
    "###### get new_seq (같은 문제로 볼 기준 정하기 -> standardized_problem_seq 정해주기)\n",
    "def generate_new_seq(row, standard):\n",
    "    standards = {\n",
    "        'big_category': ['subject', 'big_category', 'point'],\n",
    "        'middle_category': ['subject', 'big_category', 'middle_category', 'point'],\n",
    "        'small_category': ['subject', 'big_category', 'middle_category', 'small_category', 'point']}\n",
    "    if standard in standards:\n",
    "        return '-'.join(str(row[col]) for col in standards[standard])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported standard: {standard}\")\n",
    "\n",
    "first_attempts['new_seq_middle'] = first_attempts.apply(generate_new_seq, axis=1, standard=parameters.same_standard)\n",
    "representative_problem_seq = (first_attempts.groupby('new_seq_middle')['problem_seq'].apply(lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0]).reset_index())\n",
    "problem_seq_map = dict(zip(representative_problem_seq['new_seq_middle'], representative_problem_seq['problem_seq']))\n",
    "first_attempts['standardized_problem_seq'] = first_attempts['new_seq_middle'].map(problem_seq_map)\n",
    "\n",
    "##### new_seq별 너무 accuracy가 다른 response 제거   \n",
    "def remove_outliers(df):\n",
    "    Q1 = df['answer_rate'].quantile(0.25)\n",
    "    Q3 = df['answer_rate'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - parameters.IQR_range * IQR\n",
    "    upper_bound = Q3 + parameters.IQR_range * IQR\n",
    "    return df[(df['answer_rate'] >= lower_bound) & (df['answer_rate'] <= upper_bound)]\n",
    "\n",
    "first_attempts_after = first_attempts.groupby('new_seq_middle').apply(remove_outliers).reset_index(drop=True)\n",
    "columns_to_drop_candidate = ['answer_dttm', 'answer_time(문제풀이 시간)', 'ref_seq','member_exam_seq']\n",
    "columns_to_drop = [col for col in columns_to_drop_candidate if col in first_attempts_after.columns]\n",
    "preprocessing_done = first_attempts_after.drop(columns=columns_to_drop)\n",
    "preprocessing_done['correct_yn'] = preprocessing_done['correct_yn'].map({'Y': 1, 'N': 0})\n",
    "preprocessing_done = preprocessing_done.applymap(lambda x: x.replace('・', '?').replace('∙', '?') if isinstance(x, str) else x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'주희와 왕수인의 입장 비교': 1,\n",
       " '이황과 이이의 입장 비교': 1,\n",
       " '국가의 기원과 본질에 대한 관점': 1,\n",
       " '칸트, 벤담, 밀의 입장 비교': 1,\n",
       " '스피노자의 사상': 1,\n",
       " '자본주의에 대한 사상가들의 입장 비교': 1,\n",
       " '서양 사상가들의 입장 비교': 3,\n",
       " '불교 사상가들의 입장 비교': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "##### 질문지 제한 사항 (point, small_category 랭킹)\n",
    "cumul_data = {}\n",
    "for subject in subjects_list:\n",
    "    file_path = f'{parameters.original_data_path}/cumul_{subject}.csv'\n",
    "    try: \n",
    "        cumul_data[subject] = pd.read_csv(file_path, encoding='cp949')\n",
    "    except:\n",
    "        cumul_data[subject] = pd.read_csv(file_path, encoding='UTF-8')\n",
    "##### small_category 뽑기 #####\n",
    "category_structures = {}\n",
    "for subject in subjects_list:\n",
    "    with open(f'{parameters.original_data_path}/{subject}_middle_freq_list.json', 'r', encoding='UTF-8') as file:\n",
    "        category_structures[subject] = json.load(file)\n",
    "def select_frequent_small_category(data_df, category_structure):\n",
    "    selected_small_category_list = []\n",
    "    for big_category, middle_categories in category_structure.items():\n",
    "        for middle_category, freq in middle_categories.items():\n",
    "            filtered_data = data_df[(data_df['big_category'] == big_category) & \n",
    "                                    (data_df['middle_category'] == middle_category)]\n",
    "            value_counts = filtered_data['small_category'].value_counts()\n",
    "            categories = value_counts.index.tolist()\n",
    "            selections = {category: 0 for category in categories}\n",
    "            most_frequent_small_categories = []\n",
    "            for _ in range(freq):\n",
    "                max_category = max(categories, key=lambda x: value_counts[x] / (selections[x] + 1))\n",
    "                selections[max_category] += 1\n",
    "                most_frequent_small_categories.append(max_category)\n",
    "\n",
    "            selected_small_category_list.extend(most_frequent_small_categories)\n",
    "    return selected_small_category_list\n",
    "\n",
    "##### 문제 뽑기 ######\n",
    "all_selected_categories = []\n",
    "for subject, df in cumul_data.items():\n",
    "    df = df.replace(to_replace=[\"・\", \"∙\"], value=\"?\", regex=True)\n",
    "    category_structure = category_structures[subject]\n",
    "    \n",
    "    selected_categories = select_frequent_small_category(df, category_structure)\n",
    "    all_selected_categories.extend(selected_categories)\n",
    "\n",
    "\n",
    "category_counts = {category: all_selected_categories.count(category) for category in set(all_selected_categories)}\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_categories: {'서양 사상가들의 입장 비교': 3}\n",
      "normal_categories: {'주희와 왕수인의 입장 비교': [15512, 15537], '이황과 이이의 입장 비교': [15507, 15127], '국가의 기원과 본질에 대한 관점': [34771, 29196], '칸트, 벤담, 밀의 입장 비교': [33155, 29200], '스피노자의 사상': [34773, 33153], '자본주의에 대한 사상가들의 입장 비교': [34780, 35309], '불교 사상가들의 입장 비교': [15575, 29202]}\n"
     ]
    }
   ],
   "source": [
    "# Loop through each category\n",
    "set_problems_dict = {}\n",
    "total_problems = parameters.num_problems\n",
    "for category, needed_top_n in category_counts.items():\n",
    "    problems_in_category = preprocessing_done[preprocessing_done['small_category'] == category]\n",
    "    top_problems = problems_in_category.groupby('standardized_problem_seq').size().nlargest(needed_top_n * 5).index.tolist()\n",
    "    set_problems_dict[category] = top_problems\n",
    "\n",
    "special_categories = {cat: count for cat, count in category_counts.items() if count > 1}\n",
    "normal_categories = {cat: probs for cat, probs in set_problems_dict.items() if cat not in special_categories}\n",
    "print('special_categories:', special_categories)\n",
    "print('normal_categories:', normal_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Current Best: [15512, 15507, 34771, 33155, 34773, 34780, 15575, 29188, 29188, 29189] 14\n",
      "\n",
      " Current Best: [15512, 15507, 34771, 33155, 34773, 34780, 29202, 29188, 29188, 29189] 23\n"
     ]
    }
   ],
   "source": [
    "def solved_all_problems(group, required_problems):\n",
    "    solved_problems = set(group['standardized_problem_seq'])\n",
    "    return solved_problems == set(required_problems)\n",
    "\n",
    "def generate_balanced_combinations(items, count):\n",
    "    if len(items) >= count:\n",
    "        return list(itertools.combinations(items, count))\n",
    "    else:\n",
    "        base_combinations = list(itertools.combinations_with_replacement(items, count))\n",
    "        valid_combinations = []\n",
    "        for combo in base_combinations:\n",
    "            if all(combo.count(item) <= 2 for item in items):\n",
    "                valid_combinations.append(combo)\n",
    "        return valid_combinations\n",
    "\n",
    "all_combinations = []\n",
    "normal_combinations = list(itertools.product(*[normal_categories[cat] for cat in normal_categories]))\n",
    "for normal_combo in normal_combinations:\n",
    "    special_combos = [normal_combo]  \n",
    "    for special_cat, special_count in special_categories.items():\n",
    "        new_combos = []\n",
    "        for base_combo in special_combos:\n",
    "            for special_combo in generate_balanced_combinations(set_problems_dict[special_cat], special_count):\n",
    "                new_combos.append(base_combo + special_combo) \n",
    "        special_combos = new_combos \n",
    "    all_combinations.extend(special_combos) \n",
    "\n",
    "def process_combination(combination):\n",
    "    \n",
    "    problem_combination = list(combination)\n",
    "    filtered_df = preprocessing_done[preprocessing_done['standardized_problem_seq'].isin(problem_combination)]\n",
    "    solved_all = filtered_df.groupby('member_idx').filter(lambda x: solved_all_problems(x, problem_combination))\n",
    "    return len(solved_all['member_idx'].unique()), problem_combination\n",
    "max_member_count = 0\n",
    "all_top_standardized = []\n",
    "with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "    results = executor.map(process_combination, all_combinations)\n",
    "    for unique_member_count, problem_combination in results:\n",
    "        if unique_member_count > max_member_count:\n",
    "            max_member_count = unique_member_count\n",
    "            all_top_standardized = problem_combination\n",
    "            print('\\n Current Best:', all_top_standardized, max_member_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each category\n",
    "set_problems_dict = {}\n",
    "total_problems = parameters.num_problems\n",
    "\n",
    "for category, needed_top_n in category_counts.items():\n",
    "    problems_in_category = preprocessing_done[preprocessing_done['small_category'] == category]\n",
    "    top_problems = problems_in_category.groupby('standardized_problem_seq').size().nlargest(needed_top_n * 5).index.tolist()\n",
    "    set_problems_dict[category] = top_problems\n",
    "\n",
    "special_categories = {cat: count for cat, count in category_counts.items() if count > 1}\n",
    "normal_categories = {cat: probs for cat, probs in set_problems_dict.items() if cat not in special_categories}\n",
    "\n",
    "\n",
    "##### 뽑힌 문제 확인 csv 저장 #####\n",
    "results = all_top_standardized \n",
    "\n",
    "selected_df = pd.DataFrame(results, columns=['Problem Seq'])\n",
    "\n",
    "detailed_df = selected_df.merge(preprocessing_done, left_on='Problem Seq', right_on='standardized_problem_seq', how='left')\n",
    "detailed_df = detailed_df[['Problem Seq', 'subject', 'big_category', 'middle_category', 'small_category', 'point', 'new_seq_middle','title_y']]\n",
    "detailed_df = detailed_df.drop_duplicates()\n",
    "detailed_df.sort_values(['subject', 'big_category', 'middle_category', 'small_category'], inplace=True)\n",
    "response_counts = preprocessing_done.groupby('new_seq_middle').size().rename('response_count')\n",
    "correct_yn_counts = preprocessing_done.groupby('new_seq_middle')['correct_yn'].value_counts().unstack(fill_value=0)\n",
    "correct_yn_counts = correct_yn_counts.rename(columns={0: 'incorrect_count', 1: 'correct_count'})\n",
    "correct_yn_counts['response_summary'] = correct_yn_counts.apply(lambda x: f\"0:{x['incorrect_count']}, 1:{x['correct_count']}\", axis=1)\n",
    "detailed_df = detailed_df.merge(response_counts, left_on='new_seq_middle', right_index=True, how='left')\n",
    "detailed_df = detailed_df.merge(correct_yn_counts[['response_summary']], left_on='new_seq_middle', right_index=True, how='left')\n",
    "detailed_df = detailed_df[['Problem Seq', 'subject', 'title_y', 'big_category', 'middle_category', 'small_category', 'point', 'response_count', 'response_summary']]\n",
    "\n",
    "def replace_titles(group):\n",
    "    try:\n",
    "        filtered_titles = group[group['title_y'].str.contains('학년도', na=False)]['title_y']\n",
    "        if not filtered_titles.empty:\n",
    "            academic_year_title = filtered_titles.iloc[0]\n",
    "            group['title_y'] = academic_year_title\n",
    "        else:\n",
    "            raise IndexError  # Force jump to except block if no titles match\n",
    "    except IndexError:\n",
    "        if not group['title_y'].empty:\n",
    "            mode_result = group['title_y'].mode()\n",
    "            if not mode_result.empty:\n",
    "                most_frequent_title = mode_result[0]\n",
    "            else:\n",
    "                most_frequent_title = \"Default Title\"\n",
    "        else:\n",
    "            most_frequent_title = \"Default Title\"\n",
    "        group['title_y'] = most_frequent_title\n",
    "    return group\n",
    "\n",
    "detailed_df_2= detailed_df.groupby('Problem Seq').apply(replace_titles)\n",
    "detailed_df_2 = detailed_df_2.drop_duplicates(subset='Problem Seq', keep='first')\n",
    "detailed_df_2 = detailed_df_2.reset_index(drop=True)\n",
    "def calculate_response_ratio(summary):\n",
    "    pairs = summary.split(', ')\n",
    "    nums = [int(pair.split(':')[1]) for pair in pairs]\n",
    "    total = sum(nums)\n",
    "    ratios = [round(num / total, 2) for num in nums]\n",
    "    return ratios\n",
    "detailed_df_2['response_ratio'] = detailed_df_2['response_summary'].apply(calculate_response_ratio)\n",
    "\n",
    "target_counts = Counter(all_top_standardized)\n",
    "rows_to_add = []\n",
    "for problem, required_count in target_counts.items():\n",
    "    mask = detailed_df_2['Problem Seq'] == problem\n",
    "    current_count = mask.sum()  \n",
    "    additional_needed = required_count - current_count \n",
    "    if additional_needed > 0:\n",
    "        rows_to_duplicate = detailed_df_2[mask].copy() \n",
    "        for _ in range(additional_needed):\n",
    "            top_sequences = preprocessing_done[preprocessing_done['standardized_problem_seq'] == problem]['problem_seq'].value_counts().nlargest(5)\n",
    "            selected_seq = None\n",
    "            for seq, _ in top_sequences.items():\n",
    "                if seq not in detailed_df_2['Problem Seq'].values:\n",
    "                    selected_seq = seq\n",
    "                    break\n",
    "            if selected_seq is not None:\n",
    "                rows_to_duplicate['Problem Seq'] = selected_seq  \n",
    "            else:\n",
    "                rows_to_duplicate['Problem Seq'] = rows_to_duplicate['Problem Seq'].iloc[0]  \n",
    "            rows_to_add.append(rows_to_duplicate)\n",
    "if rows_to_add:\n",
    "    detailed_df_2 = pd.concat([detailed_df_2] + rows_to_add, ignore_index=True)\n",
    "# detailed_df_2.to_csv(f'./detailed/{parameters.num_problems}_{parameters.subjects}_question_detailed.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m     problem_seq_mapping_dict[problem]\u001b[38;5;241m.\u001b[39mappend(idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m## converting format to match other subjects.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m problem_seq_mapping_dict_formatted \u001b[38;5;241m=\u001b[39m {problem: seq[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m seq, problem \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28msorted\u001b[39m(problem_seq_mapping_dict\u001b[38;5;241m.\u001b[39mkeys()), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)}\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblem_seq_mapping_dict_formatted:\u001b[39m\u001b[38;5;124m'\u001b[39m, problem_seq_mapping_dict_formatted)\n\u001b[1;32m     21\u001b[0m asdf\n",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m     problem_seq_mapping_dict[problem]\u001b[38;5;241m.\u001b[39mappend(idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m## converting format to match other subjects.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m problem_seq_mapping_dict_formatted \u001b[38;5;241m=\u001b[39m {problem: \u001b[43mseq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m seq, problem \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28msorted\u001b[39m(problem_seq_mapping_dict\u001b[38;5;241m.\u001b[39mkeys()), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)}\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblem_seq_mapping_dict_formatted:\u001b[39m\u001b[38;5;124m'\u001b[39m, problem_seq_mapping_dict_formatted)\n\u001b[1;32m     21\u001b[0m asdf\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "##### Create full_data (response matrix with no re-index)\n",
    "def get_frequent_value(series):\n",
    "    value_counts = series.value_counts(dropna=True)\n",
    "    if value_counts.empty:\n",
    "        return None  \n",
    "    if len(value_counts) > 1 and value_counts.iloc[0] == value_counts.iloc[1]:\n",
    "        return series.dropna().iloc[0] if not series.dropna().empty else None\n",
    "    return value_counts.idxmax()  \n",
    "\n",
    "def map_first_occurrence(problem_seq):\n",
    "    return problem_seq_mapping_dict.get(problem_seq, [None])[0]\n",
    "\n",
    "problem_seq_mapping_dict = defaultdict(list)\n",
    "for idx, problem in enumerate(all_top_standardized):\n",
    "    problem_seq_mapping_dict[problem].append(idx + 1)\n",
    "    \n",
    "\n",
    "## converting format to match other subjects.\n",
    "# Convert defaultdict to a standard dictionary with sorted keys and flatten the lists\n",
    "standard_dict = {}\n",
    "for idx, problem in enumerate(sorted(problem_seq_mapping_dict.keys()), start=1):\n",
    "    standard_dict[problem] = idx\n",
    "\n",
    "# Display the result\n",
    "print(standard_dict)\n",
    "\n",
    "asdf\n",
    "with open(f'./processed/{parameters.num_problems}_{parameters.subjects}_problem_seq_mapping_dict.json', 'w') as file:\n",
    "        json.dump(problem_seq_mapping_dict_formatted, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15507: 1,\n",
       " 15512: 2,\n",
       " 29188: 4,\n",
       " 29189: 5,\n",
       " 29202: 6,\n",
       " 33155: 7,\n",
       " 34771: 8,\n",
       " 34773: 9,\n",
       " 34780: 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{problem: idx + 1 for idx, problem in enumerate(sorted(all_top_standardized))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {15512: [1],\n",
       "             15507: [2],\n",
       "             34771: [3],\n",
       "             33155: [4],\n",
       "             34773: [5],\n",
       "             34780: [6],\n",
       "             29202: [7],\n",
       "             29188: [8, 9],\n",
       "             29189: [10]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_seq_mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = preprocessing_done['standardized_problem_seq'].isin(all_top_standardized)\n",
    "selected_data = preprocessing_done[mask].copy()\n",
    "selected_data['standardized_problem_seq_mapped'] = selected_data['standardized_problem_seq'].apply(map_first_occurrence)\n",
    "aggregated_data = selected_data.groupby(['member_idx', 'standardized_problem_seq_mapped'])['correct_yn'].agg(get_frequent_value).reset_index()\n",
    "\n",
    "standardized_problem_seq_list = np.arange(1, len(all_top_standardized)+1)\n",
    "full_index = pd.MultiIndex.from_product(\n",
    "    [selected_data['member_idx'].unique(), standardized_problem_seq_list],\n",
    "    names=['member_idx', 'standardized_problem_seq_mapped'])\n",
    "full_data = pd.DataFrame(index=full_index).reset_index()\n",
    "full_data = full_data.merge(aggregated_data, on=['member_idx', 'standardized_problem_seq_mapped'], how='left')\n",
    "\n",
    "for problem, indices in problem_seq_mapping_dict.items():\n",
    "    if len(indices) > 1: \n",
    "        source_index = indices[0]\n",
    "        source_data = full_data[full_data['standardized_problem_seq_mapped'] == source_index][['member_idx', 'correct_yn']].set_index('member_idx')\n",
    "        \n",
    "\n",
    "        for target_index in indices[1:]:\n",
    "            condition = full_data['standardized_problem_seq_mapped'] == target_index\n",
    "            full_data.loc[condition, 'correct_yn'] = full_data.loc[condition, 'member_idx'].map(source_data['correct_yn'])\n",
    "full_data = full_data.sort_values(by=['member_idx', 'standardized_problem_seq_mapped'])\n",
    "\n",
    "full_data['original'] = np.where(full_data['correct_yn'].isna(), 'N', 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing members: 100%|██████████| 166/166 [00:00<00:00, 227.43it/s]\n",
      "Completing tasks: 100%|██████████| 166/166 [00:00<00:00, 168.45it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "##### 응답률에 따른 학생 수 보기\n",
    "def analyze_data_quality(full_data):\n",
    "    unique_problems = len({index for sublist in problem_seq_mapping_dict.values() for index in sublist})\n",
    "    results_df = pd.DataFrame(columns=[\"Threshold\", \"Remaining Member Count\", \"Filtered Out Member Count\"])\n",
    "    for threshold_percent in range(0, 100, 10):\n",
    "        threshold = unique_problems * (threshold_percent / 100.0)\n",
    "        nan_counts = full_data['correct_yn'].isna().groupby(full_data['member_idx']).sum()\n",
    "        members_to_remove = nan_counts[nan_counts > threshold].index\n",
    "        filtered_data = full_data[~full_data['member_idx'].isin(members_to_remove)]\n",
    "        member_idx_mapping = {idx: i for i, idx in enumerate(filtered_data['member_idx'].unique())}\n",
    "        filtered_data['member_idx'] = filtered_data['member_idx'].map(member_idx_mapping)\n",
    "        remaining_member_count = filtered_data['member_idx'].nunique()\n",
    "        filtered_out_member_count = full_data['member_idx'].nunique() - remaining_member_count\n",
    "        new_row = pd.DataFrame([{\n",
    "            \"Threshold\": f\"{threshold_percent}%\",\n",
    "            \"Remaining Member Count\": remaining_member_count,\n",
    "            \"Filtered Out Member Count\": filtered_out_member_count\n",
    "        }])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)   \n",
    "    return results_df\n",
    "##### 몇% 이상 푼 학생들 filter\n",
    "\n",
    "def data_filtering(full_data, threshold, num_problems):\n",
    "    score_column = 'correct_yn'\n",
    "    threshold_percent = threshold / 100.0  \n",
    "    threshold = num_problems * threshold_percent \n",
    "    nan_counts = full_data[score_column].isna().groupby(full_data['member_idx']).sum()\n",
    "    members_to_remove = nan_counts[nan_counts > threshold].index\n",
    "    filtered_data = full_data[~full_data['member_idx'].isin(members_to_remove)]\n",
    "    unique_members = pd.unique(filtered_data['member_idx'])\n",
    "    unique_members.sort()  \n",
    "    member_idx_mapping_dict = {member: idx + 1 for idx, member in enumerate(unique_members)}\n",
    "    filtered_data['member_idx_mapped'] = filtered_data['member_idx'].map(member_idx_mapping_dict)\n",
    "    filtered_data.sort_values(by=['member_idx_mapped', 'standardized_problem_seq_mapped'], inplace=True)\n",
    "    return filtered_data, member_idx_mapping_dict\n",
    "\n",
    "\n",
    "def process_member_NA1(member, full_data_merged, preprocessing_done_reduced):\n",
    "    updates = {}\n",
    "    preprocessing_done_reduced = preprocessing_done_reduced[preprocessing_done_reduced['member_idx'] == member]\n",
    "    member_data = full_data_merged[full_data_merged['member_idx'] == member]  \n",
    "    for index, row in member_data.iterrows():\n",
    "        if pd.isna(row['correct_yn']):\n",
    "            mask = (\n",
    "                (preprocessing_done_reduced['subject'] == row['subject']) &\n",
    "                (preprocessing_done_reduced['big_category'] == row['big_category']) &\n",
    "                (preprocessing_done_reduced['middle_category'] == row['middle_category'])\n",
    "            )\n",
    "            filtered_attempts = preprocessing_done_reduced[mask]\n",
    "            higher_points_correct = filtered_attempts[(filtered_attempts['point'] > row['point']) & (filtered_attempts['most_frequent'] == 1)]\n",
    "            lower_points_incorrect = filtered_attempts[(filtered_attempts['point'] < row['point']) & (filtered_attempts['most_frequent'] == 0)]\n",
    "            higher_count = len(higher_points_correct)\n",
    "            lower_count = len(lower_points_incorrect)\n",
    "            if higher_count > 0 or lower_count > 0:\n",
    "                result = 1 if higher_count > lower_count else 0\n",
    "                updates[index] = result\n",
    "    return updates\n",
    "\n",
    "def fill_nan_correct_yn(full_data, preprocessing_done, problem_seq_mapping_dict):\n",
    "    inverse_mapping = {i: problem_seq for problem_seq, indices in problem_seq_mapping_dict.items() for i in indices}\n",
    "    full_data['standardized_problem_seq'] = full_data['standardized_problem_seq_mapped'].map(inverse_mapping)\n",
    "    valid_data = preprocessing_done.dropna(subset=['correct_yn'])\n",
    "    mode_df = valid_data.groupby(['member_idx', 'standardized_problem_seq'])['correct_yn'].agg(lambda x: x.value_counts().idxmax()).reset_index()\n",
    "    mode_df.columns = ['member_idx', 'standardized_problem_seq', 'most_frequent']\n",
    "    preprocessing_done_reduced = preprocessing_done.drop_duplicates(subset=['member_idx', 'problem_seq'])[['member_idx', 'problem_seq', 'correct_yn', 'subject',\n",
    "       'big_category', 'middle_category', 'point','standardized_problem_seq']]\n",
    "    preprocessing_done_reduced = preprocessing_done_reduced.merge(mode_df, left_on=['member_idx', 'standardized_problem_seq'], right_on=['member_idx', 'standardized_problem_seq'], how='left')\n",
    "    full_data_merged = full_data.merge(\n",
    "        preprocessing_done_reduced.drop_duplicates(subset=['standardized_problem_seq'], keep='first'), left_on=['standardized_problem_seq'], right_on=['standardized_problem_seq'], how='left', suffixes=('', '_drop'))\n",
    "    full_data_merged.drop([col for col in full_data_merged.columns if 'drop' in col], axis=1, inplace=True)\n",
    "    members = full_data_merged['member_idx'].unique()\n",
    "    with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "        futures = {executor.submit(process_member_NA1, member, full_data_merged, preprocessing_done_reduced): member for member in members}\n",
    "        for future in futures:\n",
    "            member_updates = future.result()\n",
    "            for index, value in member_updates.items():\n",
    "                if pd.isna(full_data_merged.at[index, 'correct_yn']):\n",
    "                    full_data_merged.at[index, 'correct_yn'] = value\n",
    "    return full_data_merged[['member_idx', 'standardized_problem_seq_mapped', 'correct_yn','original']]\n",
    "\n",
    "##### Get p_thetas for information function. \n",
    "if set(subjects_list) & set(['독서','문학']): \n",
    "    standard = 'problem_seq'\n",
    "else:\n",
    "    standard = 'standardized_problem_seq'\n",
    "    \n",
    "preprocessing_done_answer_rate = preprocessing_done[\n",
    "    preprocessing_done[standard].isin(problem_seq_mapping_dict.keys())]\n",
    "answer_rate_per_standard_problem_seq = preprocessing_done_answer_rate.groupby(standard)['answer_rate'].mean()/100\n",
    "p_theta_problem = defaultdict(list)\n",
    "for problem_seq in problem_seq_mapping_dict:\n",
    "    answer_rate = answer_rate_per_standard_problem_seq.get(problem_seq, np.nan)\n",
    "    for index in problem_seq_mapping_dict[problem_seq]:\n",
    "        p_theta_problem[index].append(answer_rate)\n",
    "\n",
    "for key in list(p_theta_problem):\n",
    "    p_theta_problem[key] = np.nanmean(p_theta_problem[key])  # Compute mean if multiple values exist, handling NaNs\n",
    "\n",
    "p_theta_problem=dict(p_theta_problem)\n",
    "with open(f\"./detailed/10_{parameters.subjects}_p_theta_problem.json\", 'w') as json_file:\n",
    "    json.dump(p_theta_problem, json_file, indent=4)  \n",
    "\n",
    "filtered_data_0, _= data_filtering(full_data,threshold=30, num_problems=len(all_top_standardized)) # NaN 비율이 30%가 넘어가면\n",
    "\n",
    "prob_df = filtered_data_0[filtered_data_0['correct_yn'].isin([0, 1])]\n",
    "group_counts = prob_df.groupby('standardized_problem_seq_mapped').size()\n",
    "\n",
    "prob_df = prob_df.groupby('standardized_problem_seq_mapped')['correct_yn'].agg(\n",
    "    count='count',\n",
    "    probability=lambda x: (x == 1).sum() / x.count()\n",
    ").reset_index()\n",
    "\n",
    "p_theta_response_temp = prob_df.set_index('standardized_problem_seq_mapped')['probability'].to_dict()\n",
    "p_theta_response = {}\n",
    "for seq, indices in problem_seq_mapping_dict.items():\n",
    "    for index in indices:\n",
    "        if index in p_theta_response_temp:\n",
    "            p_theta_response[index] = p_theta_response_temp[index]\n",
    "    missing_indices = [index for index in indices if index not in p_theta_response]\n",
    "    if missing_indices:\n",
    "        existing_indices = [index for index in indices if index in p_theta_response]\n",
    "        if existing_indices:\n",
    "            source_index = existing_indices[0]\n",
    "            for missing_index in missing_indices:\n",
    "                p_theta_response[missing_index] = p_theta_response[source_index]\n",
    "                \n",
    "with open(f\"./detailed/10_{parameters.subjects}_p_theta_response.json\", 'w') as json_file:\n",
    "    json.dump(p_theta_response, json_file, indent=4)  \n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "num_students_by_NaN_ratio = analyze_data_quality(full_data)\n",
    "num_students_by_NaN_ratio.to_csv(f'./detailed/{parameters.num_problems}_{parameters.subjects}_NaN_ratio_analyze_original.csv', encoding='utf-8-sig', index=False)\n",
    "######\n",
    "filled_data1 = fill_nan_correct_yn(full_data, preprocessing_done, problem_seq_mapping_dict)\n",
    "analyze_data=analyze_data_quality(filled_data1)\n",
    "analyze_data.to_csv(f'./detailed/{parameters.num_problems}_{parameters.subjects}_NaN_ratio_analyze_afterNA1.csv', encoding='utf-8-sig', index=False)\n",
    "filtered_data_1,  member_idx_mapping_dict_1= data_filtering(filled_data1,threshold=30, num_problems=len(all_top_standardized)) # NaN 비율이 30%가 넘어가면\n",
    "\n",
    "filtered_data_1[['member_idx_mapped','standardized_problem_seq_mapped','correct_yn','original']].to_csv(f'./processed/{parameters.num_problems}_{parameters.subjects}_30_ratio_afterNA1.csv', encoding='utf-8-sig', index=False)\n",
    "member_idx_mapping_dict_1 = {int(k): v for k, v in member_idx_mapping_dict_1.items()}\n",
    "with open(f'./processed/{parameters.num_problems}_{parameters.subjects}_30_ratio_afterNA1_member_idx_mapping_dict_1.json', 'w') as file:\n",
    "        json.dump(member_idx_mapping_dict_1, file)\n",
    "        \n",
    "##### Fill NA 2. rule3def calculate_similarity(series1, series2):\n",
    "def calculate_similarity(series1, series2):\n",
    "    where_are_nans = np.logical_or(np.isnan(series1), np.isnan(series2))\n",
    "    series1 = series1[~where_are_nans]\n",
    "    series2 = series2[~where_are_nans]\n",
    "    return (series1 == series2).sum()\n",
    "\n",
    "\n",
    "def process_member_NA2(member, filled_data, non_nan_members):\n",
    "    member_data = filled_data[filled_data['member_idx'] == member]\n",
    "    nan_problems = member_data[member_data['correct_yn'].isna()]['standardized_problem_seq_mapped'].unique()\n",
    "    results = []\n",
    "    for problem in nan_problems:\n",
    "        member_responses = member_data[member_data['standardized_problem_seq_mapped'] != problem]['correct_yn']\n",
    "        similarity_scores = {}\n",
    "        for other_member in non_nan_members:\n",
    "            other_member_data = filled_data[filled_data['member_idx'] == other_member]\n",
    "            other_member_responses = other_member_data[other_member_data['standardized_problem_seq_mapped'] != problem]['correct_yn']\n",
    "\n",
    "            if len(member_responses) == len(other_member_responses):\n",
    "                similarity = calculate_similarity(np.array(member_responses), np.array(other_member_responses))\n",
    "                similarity_scores[other_member] = similarity\n",
    "\n",
    "        if similarity_scores:\n",
    "            most_similar_members = [member for member, score in similarity_scores.items() if score == max(similarity_scores.values())]\n",
    "            most_similar_responses = filled_data[(filled_data['member_idx'].isin(most_similar_members)) & \n",
    "                                                  (filled_data['standardized_problem_seq_mapped'] == problem)]['correct_yn']\n",
    "            try:\n",
    "                mode_response = mode(most_similar_responses, nan_policy='omit')\n",
    "                mode_value = mode_response.mode \n",
    "            except IndexError:\n",
    "                mode_value = mode_response.mode[0] if mode_response.count[0] > 0 else np.nan\n",
    "            results.append((member, problem, mode_value))\n",
    "\n",
    "    return results\n",
    "def identify_non_nan_members(filled_data):\n",
    "    nan_counts = filled_data.groupby('member_idx')['correct_yn'].apply(lambda x: x.isna().sum())\n",
    "    non_nan_members = nan_counts[nan_counts <= len(all_top_standardized)*0.4].index\n",
    "    return non_nan_members\n",
    "\n",
    "def fill_remaining_nans(filled_data):\n",
    "    filled_data.sort_values(by=['member_idx', 'standardized_problem_seq_mapped'], inplace=True)\n",
    "    nan_members = filled_data[filled_data['correct_yn'].isna()]['member_idx'].unique()\n",
    "    candidate_members = identify_non_nan_members(filled_data)\n",
    "    non_nan_members = np.setdiff1d(candidate_members, nan_members)\n",
    "    with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "        futures = {}\n",
    "        for member in tqdm(nan_members, desc=\"Processing members\"):\n",
    "            future = executor.submit(process_member_NA2, member, filled_data, non_nan_members)\n",
    "            futures[future] = member\n",
    "        for future in tqdm(futures, desc=\"Completing tasks\"):\n",
    "            results = future.result()\n",
    "            for member, problem, value in results:\n",
    "                condition = ((filled_data['member_idx'] == member) & \n",
    "                             (filled_data['standardized_problem_seq_mapped'] == problem))\n",
    "                filled_data.loc[condition & filled_data['correct_yn'].isna(), 'correct_yn'] = value\n",
    "\n",
    "    return filled_data\n",
    "\n",
    "filled_data2 = fill_remaining_nans(filled_data1)\n",
    "analyze_data=analyze_data_quality(filled_data2)\n",
    "analyze_data.to_csv(f'./detailed/{parameters.num_problems}_{parameters.subjects}_NaN_ratio_analyze_afterNA2.csv', encoding='utf-8-sig', index=False)\n",
    "filtered_data_2, member_idx_mapping_dict_2 = data_filtering(filled_data2,threshold=30,num_problems=len(all_top_standardized)) # NaN 비율이 30%가 넘어가면\n",
    "filtered_data_2[['member_idx_mapped','standardized_problem_seq_mapped','correct_yn','original']].to_csv(f'./processed/{parameters.num_problems}_{parameters.subjects}_30_ratio_afterNA2.csv', encoding='utf-8-sig', index=False)\n",
    "member_idx_mapping_dict_2 = {int(k): v for k, v in member_idx_mapping_dict_2.items()}\n",
    "with open(f'./processed/{parameters.num_problems}_{parameters.subjects}_30_ratio_afterNA1_member_idx_mapping_dict_2.json', 'w') as file:\n",
    "        json.dump(member_idx_mapping_dict_2, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lanchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
